{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[По материалам открытого курса машинного обучения (Юрий Кашиницкий)](https://habr.com/ru/company/ods/blog/322534/)\n",
    "### Плюсы и минусы деревьев решений и метода ближайших соседей\n",
    "\n",
    "#### Плюсы и минусы деревьев решений\n",
    "\n",
    "***Плюсы:***\n",
    "\n",
    "- Порождение четких правил классификации, понятных человеку, например, \"если возраст < 25 и интерес к мотоциклам, то отказать в кредите\". Это свойство называют интерпретируемостью модели;\n",
    "- Деревья решений могут легко визуализироваться, то есть может \"интерпретироваться\" (строгого определения я не видел) как сама модель (дерево), так и прогноз для отдельного взятого тестового объекта (путь в дереве);\n",
    "- Быстрые процессы обучения и прогнозирования;\n",
    "- Малое число параметров модели;\n",
    "- Поддержка и числовых, и категориальных признаков.\n",
    "\n",
    "***Минусы:***\n",
    "\n",
    "- У порождения четких правил классификации есть и другая сторона: деревья очень чувствительны к шумам во входных данных, вся модель может кардинально измениться, если немного изменится обучающая выборка (например, если убрать один из признаков или добавить несколько объектов), поэтому и правила классификации могут сильно изменяться, что ухудшает интерпретируемость модели;\n",
    "- Разделяющая граница, построенная деревом решений, имеет свои ограничения (состоит из гиперплоскостей, перпендикулярных какой-то из координатной оси), и на практике дерево решений по качеству классификации уступает некоторым другим методам;\n",
    "- Необходимость отсекать ветви дерева (pruning) или устанавливать минимальное число элементов в листьях дерева или максимальную глубину дерева для борьбы с переобучением. Впрочем, переобучение — проблема всех методов машинного обучения;\n",
    "- Нестабильность. Небольшие изменения в данных могут существенно изменять построенное дерево решений. С этой проблемой борются с помощью ансамблей деревьев решений (рассмотрим далее);\n",
    "- Проблема поиска оптимального дерева решений (минимального по размеру и способного без ошибок классифицировать выборку) NP-полна, поэтому на практике используются эвристики типа жадного поиска признака с максимальным приростом информации, которые не гарантируют нахождения глобально оптимального дерева;\n",
    "- Сложно поддерживаются пропуски в данных. Friedman оценил, что на поддержку пропусков в данных ушло около 50% кода CART (классический алгоритм построения деревьев классификации и регрессии – Classification And Regression Trees, в sklearn реализована улучшенная версия именно этого алгоритма);\n",
    "- Модель умеет только интерполировать, но не экстраполировать (это же верно и для леса и бустинга на деревьях). То есть дерево решений делает константный прогноз для объектов, находящихся в признаковом пространстве вне параллелепипеда, охватывающего все объекты обучающей выборки. В нашем примере с желтыми и синими шариками это значит, что модель дает одинаковый прогноз для всех шариков с координатой > 19 или < 0.\n",
    "\n",
    "#### Плюсы и минусы метода ближайших соседей\n",
    "\n",
    "***Плюсы:***\n",
    "\n",
    "- Простая реализация;\n",
    "- Неплохо изучен теоретически;\n",
    "- Как правило, метод хорош для первого решения задачи, причем не только классификации или регрессии, но и, например, рекомендации;\n",
    "- Можно адаптировать под нужную задачу выбором метрики или ядра (в двух словах: ядро может задавать операцию сходства для сложных объектов типа графов, а сам подход kNN остается тем же).;\n",
    "- Неплохая интерпретация, можно объяснить, почему тестовый пример был классифицирован именно так. Хотя этот аргумент можно атаковать: если число соседей большое, то интерпретация ухудшается (условно: \"мы не дали ему кредит, потому что он похож на 350 клиентов, из которых 70 – плохие, что на 12% больше, чем в среднем по выборке\").\n",
    "\n",
    "***Минусы:***\n",
    "\n",
    "- Метод считается быстрым в сравнении, например, с композициями алгоритмов, но в реальных задачах, как правило, число соседей, используемых для классификации, будет большим (100-150), и в таком случае алгоритм будет работать не так быстро, как дерево решений;\n",
    "- Если в наборе данных много признаков, то трудно подобрать подходящие веса и определить, какие признаки не важны для классификации/регрессии;\n",
    "- Зависимость от выбранной метрики расстояния между примерами. Выбор по умолчанию евклидового расстояния чаще всего ничем не обоснован. Можно отыскать хорошее решение перебором параметров, но для большого набора данных это отнимает много времени;\n",
    "- Нет теоретических оснований выбора определенного числа соседей — только перебор (впрочем, чаще всего это верно для всех гиперпараметров всех моделей). В случае малого числа соседей метод чувствителен к выбросам, то есть склонен переобучаться;\n",
    "- Как правило, плохо работает, когда признаков много, из-за \"прояклятия размерности\". Про это хорошо рассказывает известный в ML-сообществе профессор Pedro Domingos – тут в популярной статье \"A Few Useful Things to Know about Machine Learning\", также \"the curse of dimensionality\" описывается в книге Deep Learning в главе \"Machine Learning basics\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": false,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
